\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax 
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {2.1}{\ignorespaces A 2-dimensional example of different possible separating hyperplanes that correctly classify all the toy data points.}}{16}{figure.2.1}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {2.2}{\ignorespaces An illustration of the soft margin SVM solution on an example $2$-dimensional non-linearly separable dataset.}}{16}{figure.2.2}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {2.3}{\ignorespaces Vapnik's $\epsilon $-insensitivity loss function.}}{19}{figure.2.3}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {2.4}{\ignorespaces Support vector regression example solution.}}{19}{figure.2.4}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.1}{\ignorespaces SVR Flow Diagram. Firstly, the SVR method divides the MT dataset into $m$ ST datasets, $\mathcal {D}_1, \mathcal {D}_2, \ldots , \mathcal {D}_m$. It then independently trains models, $h_1, h_2, \ldots , h_m$, for each ST dataset.}}{26}{figure.3.1}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.2}{\ignorespaces SVRRC Flow Diagram on a dataset with 3 targets. SVRRC first builds the $6$ random chains of the target's indices ($3$ examples are shown). It then constructs a chained model by proceeding recursively over the chain, building a model, and appending the current target to the input space to predict the next target in the chain. }}{28}{figure.3.2}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.3}{\ignorespaces SVRCC Flow Diagram on a sample dataset with 3 targets. SVRCC first finds the direction of maximum correlation among the targets and uses that order as the only chain. It then constructs the chained model as done in SVRRC. }}{29}{figure.3.3}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.4}{\ignorespaces Bonferroni-Dunn test for aCC}}{34}{figure.3.4}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.5}{\ignorespaces Bonferroni-Dunn test for MSE}}{35}{figure.3.5}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.6}{\ignorespaces Bonferroni-Dunn test for aRMSE}}{37}{figure.3.6}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.7}{\ignorespaces Bonferroni-Dunn test for aRRMSE}}{38}{figure.3.7}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.8}{\ignorespaces Bonferroni-Dunn test for Run Time}}{40}{figure.3.8}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {4.1}{\ignorespaces \relax \fontsize {10.95}{13.6}\selectfont \abovedisplayskip 11pt plus3pt minus6pt\belowdisplayskip \abovedisplayskip \abovedisplayshortskip \z@ plus3pt\belowdisplayshortskip 6.5pt plus3.5pt minus3pt \def \parsep 4.5pt plus 2pt minus 1pt \itemsep \parsep \topsep 9pt plus 3pt minus 5pt{\parsep 4.5pt plus 2pt minus 1pt \itemsep \parsep \topsep 9pt plus 3pt minus 5pt}A summary of the steps performed by MIRSVM. The representatives are first randomly initialized and continuously updated according to the current hyperplane, which is found using a quadratic programming solver. Upon completion, the model is returned along with the optimal bag-representatives.}}{51}{figure.4.1}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {4.2}{\ignorespaces Bag representative convergence plots on 9 datasets. The blue line shows the number of bag representatives that are equal from one iteration to the next. The red dashed line represents the total number of bags.}}{52}{figure.4.2}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {4.3}{\ignorespaces Difference between MIRSVM and MISVM on a random $2$-dimensional toy dataset. Note the differing number of support vectors produced by the two methods. MIRSVM has $6$, one for each bag, and MISVM has $29$. Also note the smoother representation of the data distribution given by MIRSVM's decision boundary, unlike MISVM whose decision boundary was greatly influenced by the larger number of support vectors belonging to the negative class with respect to the only $2$ positive support vectors.}}{53}{figure.4.3}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {4.4}{\ignorespaces Bonferroni-Dunn test for Accuracy}}{58}{figure.4.4}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {4.5}{\ignorespaces Bonferroni-Dunn test for Precision}}{59}{figure.4.5}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {4.6}{\ignorespaces Bonferroni-Dunn test for Recall}}{60}{figure.4.6}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {4.7}{\ignorespaces Bonferroni-Dunn test for Cohen's Kappa rate}}{61}{figure.4.7}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {4.8}{\ignorespaces Bonferroni-Dunn test for AUC}}{62}{figure.4.8}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {4.9}{\ignorespaces Bonferroni-Dunn test for overall ranks comparison}}{64}{figure.4.9}
